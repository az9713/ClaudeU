{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding HTML and BeautifulSoup\n",
    "\n",
    "This notebook walks you through HTML basics and how BeautifulSoup extracts data from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simplest Example: One Tag\n",
    "\n",
    "Here's a tiny piece of HTML with just a heading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The whole tag: <h1>Software Engineer</h1>\n",
      "Just the text: Software Engineer\n"
     ]
    }
   ],
   "source": [
    "# This is what raw HTML looks like to Python - just a string\n",
    "html = \"<h1>Software Engineer</h1>\"\n",
    "\n",
    "# Turn it into a BeautifulSoup object we can search\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Find the h1 tag\n",
    "heading = soup.find(\"h1\")\n",
    "\n",
    "print(\"The whole tag:\", heading)\n",
    "print(\"Just the text:\", heading.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happened?\n",
    "\n",
    "1. We gave BeautifulSoup a string of HTML\n",
    "2. `soup.find(\"h1\")` searched for the `<h1>` tag\n",
    "3. `.text` pulled out just the words inside (without the `<h1>` wrapper)\n",
    "\n",
    "### üß™ Experiment: Change \"h1\" to \"p\" and see what happens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multiple Tags: Finding One Specific Thing\n",
    "\n",
    "Real web pages have lots of tags. Let's make a fake job listing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Title: Machine Learning Engineer\n",
      "First paragraph: Company: TechCorp\n"
     ]
    }
   ],
   "source": [
    "html = \"\"\"\n",
    "<div>\n",
    "    <h1>Machine Learning Engineer</h1>\n",
    "    <p>Company: TechCorp</p>\n",
    "    <p>Location: San Francisco</p>\n",
    "    <p>Salary: $150,000 - $200,000</p>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Find just the job title\n",
    "title = soup.find(\"h1\")\n",
    "print(\"Job Title:\", title.text)\n",
    "\n",
    "# Find the first paragraph\n",
    "first_p = soup.find(\"p\")\n",
    "print(\"First paragraph:\", first_p.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happened?\n",
    "\n",
    "- `find()` grabs the **first** match it sees\n",
    "- Even though there are 3 `<p>` tags, `soup.find(\"p\")` only grabbed \"Company: TechCorp\"\n",
    "\n",
    "### üß™ Experiment: What if you want the salary instead? Try finding it below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn: How would you get the salary?\n",
    "# Hint: You need to find ALL the <p> tags, not just the first one\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Finding ALL Tags of One Type\n",
    "\n",
    "Use `find_all()` to grab every match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many paragraphs? 3\n",
      "\n",
      "Company: TechCorp\n",
      "Location: San Francisco\n",
      "Salary: $150,000 - $200,000\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Get ALL paragraphs\n",
    "all_paragraphs = soup.find_all(\"p\")\n",
    "\n",
    "print(\"How many paragraphs?\", len(all_paragraphs))\n",
    "print()\n",
    "\n",
    "# Loop through them\n",
    "for p in all_paragraphs:\n",
    "    print(p.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happened?\n",
    "\n",
    "- `find_all()` returns a **list** of all matching tags\n",
    "- You can loop through the list to process each one\n",
    "\n",
    "### üß™ Experiment: Print only the salary (the 3rd paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn: Get just the 3rd paragraph\n",
    "# Hint: Lists use numbers starting at 0, so the 3rd item is index [2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tags with Classes: Finding Specific Elements\n",
    "\n",
    "Real websites label tags with **classes** so different things can be styled differently.\n",
    "\n",
    "Think of classes like name tags at a conference‚Äîlots of people, but you can find \"all the speakers\" by their name tag color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 job links:\n",
      "Text: AI Engineer\n",
      "URL: /job/123\n",
      "\n",
      "Text: Data Scientist\n",
      "URL: /job/456\n",
      "\n"
     ]
    }
   ],
   "source": [
    "html = \"\"\"\n",
    "<div>\n",
    "    <a class=\"job-link\" href=\"/job/123\">AI Engineer</a>\n",
    "    <a class=\"job-link\" href=\"/job/456\">Data Scientist</a>\n",
    "    <a class=\"other-link\" href=\"/about\">About Us</a>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Find all links with class=\"job-link\"\n",
    "job_links = soup.find_all(\"a\", class_=\"job-link\")\n",
    "\n",
    "print(\"Found\", len(job_links), \"job links:\")\n",
    "for link in job_links:\n",
    "    print(\"Text:\", link.text)\n",
    "    print(\"URL:\", link[\"href\"])  # Get the href attribute\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happened?\n",
    "\n",
    "1. `find_all(\"a\", class_=\"job-link\")` finds ONLY the `<a>` tags with that specific class\n",
    "2. The \"About Us\" link was ignored because it has `class=\"other-link\"`\n",
    "3. `link[\"href\"]` grabs the URL from the tag (like getting a dictionary value)\n",
    "\n",
    "### üß™ Experiment: What happens if you search for class=\"other-link\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn: Find the \"other-link\" and print its text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Connecting to Your Job Scraper\n",
    "\n",
    "Here's the actual line from your `example.ipynb` that finds job links:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job URLs found:\n",
      "https://aijobs.ai/job/123\n",
      "https://aijobs.ai/job/456\n",
      "https://aijobs.ai/job/789\n"
     ]
    }
   ],
   "source": [
    "# From your actual code:\n",
    "# job_cards = soup.find_all(\"a\", class_=\"jobcardStyle1\")\n",
    "\n",
    "# Let's simulate what that does:\n",
    "fake_aijobs_html = \"\"\"\n",
    "<div>\n",
    "    <a class=\"jobcardStyle1\" href=\"https://aijobs.ai/job/123\">ML Engineer at Google</a>\n",
    "    <a class=\"jobcardStyle1\" href=\"https://aijobs.ai/job/456\">AI Researcher at OpenAI</a>\n",
    "    <a class=\"jobcardStyle1\" href=\"https://aijobs.ai/job/789\">Data Scientist at Meta</a>\n",
    "    <a class=\"navigation-link\" href=\"/about\">About</a>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(fake_aijobs_html, \"html.parser\")\n",
    "job_cards = soup.find_all(\"a\", class_=\"jobcardStyle1\")\n",
    "\n",
    "# Extract just the URLs (this is what your code does next)\n",
    "job_urls = [a[\"href\"] for a in job_cards]\n",
    "\n",
    "print(\"Job URLs found:\")\n",
    "for url in job_urls:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happened?\n",
    "\n",
    "1. Found all `<a>` tags with `class=\"jobcardStyle1\"` (the job postings)\n",
    "2. Ignored the \"About\" link because it has a different class\n",
    "3. Used a **list comprehension** `[a[\"href\"] for a in job_cards]` to extract just the URLs\n",
    "\n",
    "This is exactly what your scraper does on the real aijobs.ai website!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèãÔ∏è Practice Exercise\n",
    "\n",
    "Below is HTML for 3 job postings. Extract:\n",
    "1. All job titles\n",
    "2. All salaries\n",
    "3. Just the location from the second job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "practice_html = \"\"\"\n",
    "<div>\n",
    "    <div class=\"job-card\">\n",
    "        <h2 class=\"job-title\">Senior Data Engineer</h2>\n",
    "        <p class=\"company\">Stripe</p>\n",
    "        <p class=\"location\">Seattle, WA</p>\n",
    "        <p class=\"salary\">$180,000 - $220,000</p>\n",
    "    </div>\n",
    "    <div class=\"job-card\">\n",
    "        <h2 class=\"job-title\">AI Research Scientist</h2>\n",
    "        <p class=\"company\">DeepMind</p>\n",
    "        <p class=\"location\">London, UK</p>\n",
    "        <p class=\"salary\">¬£120,000 - ¬£180,000</p>\n",
    "    </div>\n",
    "    <div class=\"job-card\">\n",
    "        <h2 class=\"job-title\">ML Platform Engineer</h2>\n",
    "        <p class=\"company\">Anthropic</p>\n",
    "        <p class=\"location\">San Francisco, CA</p>\n",
    "        <p class=\"salary\">$200,000 - $280,000</p>\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(practice_html, \"html.parser\")\n",
    "\n",
    "# Your turn:\n",
    "# 1. Find all job titles and print them\n",
    "\n",
    "\n",
    "# 2. Find all salaries and print them\n",
    "\n",
    "\n",
    "# 3. Find just the location from the 2nd job\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° Solution (Try yourself first!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Titles:\n",
      "- Senior Data Engineer\n",
      "- AI Research Scientist\n",
      "- ML Platform Engineer\n",
      "\n",
      "Salaries:\n",
      "- $180,000 - $220,000\n",
      "- ¬£120,000 - ¬£180,000\n",
      "- $200,000 - $280,000\n",
      "\n",
      "2nd job location: London, UK\n"
     ]
    }
   ],
   "source": [
    "# Solution 1: All job titles\n",
    "titles = soup.find_all(\"h2\", class_=\"job-title\")\n",
    "print(\"Job Titles:\")\n",
    "for title in titles:\n",
    "    print(\"-\", title.text)\n",
    "\n",
    "print()\n",
    "\n",
    "# Solution 2: All salaries\n",
    "salaries = soup.find_all(\"p\", class_=\"salary\")\n",
    "print(\"Salaries:\")\n",
    "for salary in salaries:\n",
    "    print(\"-\", salary.text)\n",
    "\n",
    "print()\n",
    "\n",
    "# Solution 3: Just the 2nd location\n",
    "locations = soup.find_all(\"p\", class_=\"location\")\n",
    "second_location = locations[1]  # Remember: 0 = first, 1 = second\n",
    "print(\"2nd job location:\", second_location.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **HTML** = labeled boxes of content (`<h1>`, `<p>`, `<a>`, etc.)\n",
    "2. **BeautifulSoup** = tool to search through HTML and extract specific pieces\n",
    "3. **`find()`** = get the first match\n",
    "4. **`find_all()`** = get all matches (returns a list)\n",
    "5. **Classes** = labels that help you find specific elements\n",
    "6. **`.text`** = get just the words (no HTML tags)\n",
    "7. **`tag[\"href\"]`** = get an attribute (like a URL)\n",
    "\n",
    "Your job scraper uses these same techniques to automatically extract data from hundreds of job postings!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
